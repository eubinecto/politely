import inspect
import os
import re
import requests  # noqa
import pandas as pd  # noqa
import streamlit as st
from khaiii.khaiii import KhaiiiApi
from typing import Any, Tuple, Set
from soynlp.hangle import compose, decompose
from politely.fetchers import fetch_honorifics, fetch_rules
from politely.errors import EFNotIncludedError, EFNotSupportedError
from multipledispatch import dispatch
from dataclasses import dataclass, field
from soynlp.lemmatizer import conjugate as soynlp_conjugate


class Styler:
    """
    A rule-based Korean Politeness Styler
    """
    @dataclass
    class Logs:
        args: dict = field(default_factory=dict)
        case: dict = field(default_factory=dict)
        steps: list = field(default_factory=list)
        honorifics: Set[Tuple[str, str]] = field(default_factory=set)
        conjugations: Set[Tuple[str, str, str, str]] = field(default_factory=set)
    # class-owned attributes
    RULES: dict = fetch_rules()
    HONORIFICS: dict = fetch_honorifics()

    def __init__(self):
        # object-owned attributes
        self.khaiii = KhaiiiApi()
        self.out: Any = None
        self.logs = self.Logs()

    @dispatch(str, str, str)
    def __call__(self, sent: str, listener: str, environ: str) -> str:
        """
        First way of using Styler - have Styler determine the politeness for you.
        """
        self.clear() \
            .save() \
            .determine(listener, environ) \
            .process(sent, self.logs.case['politeness'])
        return self.out

    @dispatch(str, int)
    def __call__(self, sent: str, politeness: int) -> str:
        """
        The other way of using Styler - you know your politeness already, just use this as a styler per se.
        This would more suitable than the first __call__ for  e.g. augmentation of data purposes.
        """
        self.clear() \
            .save() \
            .process(sent, politeness)
        return self.out

    def process(self, sent: str, politeness: int):
        """
        The common steps for all __call__'s.
        """
        self.preprocess(sent) \
            .analyze() \
            .check() \
            .log() \
            .honorify(politeness) \
            .log() \
            .conjugate() \
            .log()
        return self

    def save(self):
        """
        save whatever arguments that were provided to __call__
        """
        f_back = inspect.currentframe().f_back
        args = inspect.getargvalues(f_back)
        self.logs.args.update(args.locals)
        return self

    def clear(self):
        """
        clear all logs
        """
        self.logs.args.clear()
        self.logs.steps.clear()
        self.logs.honorifics.clear()
        self.logs.conjugations.clear()
        return self

    def log(self):
        """
        log the current out
        """
        self.logs.steps.append(self.out)
        return self

    def determine(self, listener: str, environ: str):
        """
        determine the case from the rules.
        """
        self.logs.case = self.RULES[listener][environ]
        return self

    def preprocess(self, sent: str):
        self.out = sent.strip()  # khaiii model is sensitive to empty spaces, so we should get rid of it.
        if not self.out.endswith("?") and not self.out.endswith("!"):
            self.out = self.out + "." if not self.out.endswith(".") else self.out  # for accurate pos-tagging
        return self

    def analyze(self):
        tokens = self.khaiii.analyze(self.out)
        self.out = tokens
        return self

    def check(self):
        """
        Check if your assumption holds. Raises a custom error if any of them does not hold.
        """
        efs = [
            "+".join(map(str, token.morphs))
            for token in self.out
            if "EF" in "+".join(map(str, token.morphs))
        ]
        # assumption 1: the sentence must include more than 1 EF's
        if not efs:
            raise EFNotIncludedError("|".join(["+".join(map(str, token.morphs)) for token in self.out]))
        # assumption 2: all EF's should be supported by KPS.
        for ef in efs:
            for pattern in self.HONORIFICS.keys():
                if self.matched(pattern, ef):
                    break
            else:
                raise EFNotSupportedError(ef)
        return self

    def honorify(self, politeness: int):
        lex2morphs = [(token.lex, list(map(str, token.morphs))) for token in self.out]
        self.out = list()
        for lex, morphs in lex2morphs:
            tuned = "+".join(morphs)
            for pattern in self.HONORIFICS.keys():
                if self.matched(pattern, tuned):
                    honorific = self.HONORIFICS[pattern][politeness]
                    tuned = tuned.replace(pattern, honorific)
                    self.logs.honorifics.add((pattern, honorific))
            # if something has changed, then go for it, but otherwise just use the lex.
            before = [morph.split("/")[0] for morph in morphs]
            after = [morph.split("/")[0] for morph in tuned.split("+")]
            if "".join(before) != "".join(after):
                self.out.append(after)
            else:
                self.out.append(lex)
        return self

    def conjugate(self):
        """
        conjugate L -> R.
        Custom rules are followed by soynlp's rules.
        """
        out = list()
        for chunk in self.out:
            if isinstance(chunk, list):
                # you should conjugate these
                left = chunk[0]
                for i in range(len(chunk) - 1):
                    right = chunk[i + 1]
                    r_first = right[0]
                    l_last = left[-1]
                    l_cho, l_jung, l_jong = decompose(l_last)  # decompose the last element
                    r_cho, r_jung, r_jong = decompose(r_first)  # decompose the first element
                    if l_jong == " " and right.startswith("„ÖÇÎãà"):
                        # e.g. Ï†Ñ Ïù¥Ï†ú Îñ†ÎÇò„ÖÇÎãàÎã§ -> Ï†Ñ Ïù¥Ï†ú Îñ†ÎÇ©ÎãàÎã§
                        left = left[:-1] + compose(l_cho, l_jung, "„ÖÇ")
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"Ïñ¥Í∞ÑÏóê Î∞õÏπ®Ïù¥ ÏóÜÍ≥† Ïñ¥ÎØ∏Í∞Ä ÏùçÏù∏ Í≤ΩÏö∞, „ÖÇÏùÄ Ïñ¥Í∞ÑÏùò Î∞õÏπ®ÏúºÎ°ú Ïì∞ÏûÑ"))
                    elif l_jong != " " and right.startswith("„ÖÇÎãà"):
                        # e.g. Í∞î„ÖÇÎãàÎã§ -> Í∞îÏäµÎãàÎã§
                        left += f"Ïäµ{right[1:]}"
                        self.logs.conjugations.add((l_last, r_first, left, f"Ï¢ÖÏÑ±ÏûàÏùå + `„ÖÇÎãà` -> ÏäµÎãà"))
                    elif l_jong != " " and right.startswith("„ÖÇÏãú"):
                        # Ï§çÏùÄ ÏòàÏô∏
                        if left == "Ï§ç":
                            left = left[:-1] + "Ï£ºÏõÅ"
                            left += right[1:]
                            self.logs.conjugations.add((l_last, r_first, left, f"Ï§ç ÏòàÏô∏"))
                        else:
                            # e.g. Î®π„ÖÇÏãúÎã§
                            left += f"Ïùç{right[1:]}"
                            self.logs.conjugations.add((l_last, r_first, left, f"Ï¢ÖÏÑ±ÏûàÏùå + `„ÖÇÏãú` -> ÏùçÎãà"))
                    elif l_jong == "„Öé" and r_first == "Ïñ¥":
                        # e.g. Ïñ¥Îñª + Ïñ¥Ïöî -> Ïñ¥ÎïåÏöî, Ï¢ãÏñ¥Ïöî -> Ï¢ãÏïÑÏöî
                        left = left[:-1] + compose(l_cho, "„Öê",  " ")
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Öé` + `Ïñ¥` -> `„Öê`"))
                    elif l_jung == "„Ö£" and l_jong == " " and r_first == "Ïñ¥":
                        # e.g. ÏãúÏñ¥ -> ÏÖî
                        # ÌïòÏßÄÎßå e.g. ÏûàÏñ¥ -> ÏûàÏñ¥
                        left = left[:-1] + compose(l_cho, "„Öï",  " ")
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Ö£`+ `„Öì` -> `„Öï`"))
                    elif l_jung == "„Öè" and l_jong in ("„Ñ∑", "„Öå") and r_first == "Ïñ¥":
                        # e.g. Í∞ôÏñ¥Ïöî -> Í∞ôÏïÑÏöî
                        # e.g  Îã´Ïñ¥Ïöî -> Îã´ÏïÑÏöî
                        left += f"ÏïÑ{right[1:]}"
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Öè (Ï¢ÖÏÑ±o)`+ `„Öì` -> `„Öï`"))
                    elif l_last == "Ìïò" and r_jung in ("„Öì", "„Öï"):
                        # e.g. ÌïòÏñ¥Ïöî -> Ìï¥Ïöî, ÌïòÏó¨Ïöî -> Ìï¥Ïöî, ÌïòÏóàÏñ¥Ïöî -> ÌñàÏñ¥Ïöî  -> ÌïòÏòÄÏñ¥Ïöî -> ÌñàÏñ¥Ïöî
                        left = left[:-1] + compose(l_cho, "„Öê",  r_jong)
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"`Ìïò`+ (`„Öì` ÎòêÎäî `„Öï`) -> `Ìï¥`"))
                    elif l_jung == "„Öè" and r_first == "Ïùò":
                        # e.g. ÎÇòÏùò -> ÎÇ¥ ("ÎÇ¥"Í∞Ä Îçî ÎßéÏù¥ Ïì∞Ïù¥ÎØÄÎ°ú)
                        left = left[:-1] + compose(l_cho, "„Öê",  " ")
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Öè`+ `Ïùò` -> `„Öê`"))
                    elif l_jung == "„Öì" and r_first == "Ïùò":
                        # e.g. Ï†ÄÏùò -> Ï†ú ("Ï†ú"Í∞Ä Îçî ÎßéÏù¥ Ïì∞Ïù¥ÎØÄÎ°ú)
                        left = left[:-1] + compose(l_cho, "„Öî",  " ")
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Öì`+ `Ïùò` -> `„Öî`"))
                    elif l_jung == "„Öì" and l_jong == " " and r_first == "Ïù¥":
                        # e.g. Í±∞Ïù¥Ï£† -> Í±∞Ï£†?
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"„Öì+ Ïù¥ -> „Öì (Ïù¥ ÌÉàÎùΩ)"))
                    elif l_jong == '„Ñ∑' and r_cho == "„Öá":
                        # e.g. Íπ®Îã´ÏïÑ -> Íπ®Îã¨ÏïÑ
                        left = left[:-1] + compose(l_cho, l_jung,  "„Ñπ")
                        left += right
                        self.logs.conjugations.add((l_last, r_first, left, f"`„Ñ∑` Ï¢ÖÏÑ± + `„Öá` Ï¥àÏÑ± -> `„Ñπ` Ï¢ÖÏÑ±"))
                    elif l_jung == "„Öè" and l_jong == " " and r_first == "Ïñ¥":
                        left += right[1:]
                        self.logs.conjugations.add((l_last, r_first, left, f"ÎèôÎ™®Ïùå ÌÉàÎùΩ"))
                    else:
                        # rely on soynlp for the remaining cases
                        # always pop the shortest one (e.g. ÎßàÏãúÏñ¥, ÎßàÏÖî, Îëò Ï§ë ÌïòÎÇòÏùº Í≤ΩÏö∞ ÎßàÏÖîÎ•º ÏÑ†ÌÉù)
                        # warning - popping an element from the set maybe non-deterministic
                        left = min(soynlp_conjugate(left, right), key=lambda x: len(x))
                        self.logs.conjugations.add((l_last, r_first, left, f"conjugations done by soynlp"))
                # after the for loop ends
                out.append(left)
            else:
                out.append(chunk)
        self.out = " ".join(out)
        return self

    # --- accessing options --- #
    @property
    def listeners(self):
        return pd.DataFrame(self.RULES).transpose().index

    @property
    def environs(self):
        return pd.DataFrame(self.RULES).transpose().columns

    # --- supporting methods --- #
    @staticmethod
    def matched(pattern: str, string: str) -> bool:
        return True if re.match(f"(^|.*\\+){re.escape(pattern)}(\\+.*|$)", string) else False


class Translator:
    def __call__(self, sent: str) -> str:
        url = "https://openapi.naver.com/v1/papago/n2mt"
        headers = {
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "X-Naver-Client-Id": os.environ['NAVER_CLIENT_ID'],
            "X-Naver-Client-Secret": os.environ['NAVER_CLIENT_SECRET']
        }
        data = {
            "source": "en",
            "target": "ko",
            "text": sent,
        }
        r = requests.post(url, headers=headers, data=data)
        r.raise_for_status()
        return r.json()['message']['result']['translatedText']


class Explainer:
    """
    This is here to explain each step in tuner. (mainly - apply_honorifics, apply_abbreviations, apply_irregulars).
    It is given a tuner as an input, attempts to explain the latest process.
    """

    def __init__(self, logs: Styler.Logs):
        self.logs = logs

    def __call__(self):
        # CSS to inject contained in a string
        hide_table_row_index = """
                    <style>
                    tbody th {display:none}
                    .blank {display:none}
                    </style>
                    """
        # Inject CSS with Markdown
        st.markdown(hide_table_row_index, unsafe_allow_html=True)
        # --- step 1 ---
        msg_1 = "### 1Ô∏è‚É£ Politeness"
        politeness = self.logs.case['politeness']
        politeness = "casual style (-Ïñ¥)" if politeness == 1 \
            else "polite style (-Ïñ¥Ïöî)" if politeness == 2 \
            else "formal style (-ÏäµÎãàÎã§)"
        reason = self.logs.case['reason']
        msg_1 += f"\nYou should speak in a `{politeness}` to your `{self.logs.args['listener']}`" \
                 f" when you are in a `{self.logs.args['environ']}` environment."
        msg_1 += f"\n\n Why so? {reason}"
        st.markdown(msg_1)
        # --- step 2 ---
        msg_2 = f"### 2Ô∏è‚É£ Morphemes"
        before = self.logs.args['sent'].split(" ")
        after = ["".join(list(map(str, token.morphs))) for token in self.logs.steps[0]]
        df = pd.DataFrame(zip(before, after), columns=['before', 'after'])
        st.markdown(msg_2)
        st.markdown(df.to_markdown(index=False))
        # --- step 3 ---
        msg_3 = f"### 3Ô∏è‚É£ Honorifics"
        before = " ".join(["".join(list(map(str, token.morphs))) for token in self.logs.steps[0]])
        after = " ".join([
            "".join(elem) if isinstance(elem, list)
            else elem
            for elem in self.logs.steps[1]
        ])
        for key, val in self.logs.honorifics:
            before = before.replace(key, f"`{key}`")
            after = after.replace(val, f"`{val}`")
        df = pd.DataFrame(zip(before.split(" "), after.split(" ")), columns=['before', 'after'])
        st.markdown(msg_3)
        st.markdown(df.to_markdown(index=False))
        # # --- step 4 ---
        msg_4 = "### 4Ô∏è‚É£ Conjugations"
        st.markdown(msg_4)
        st.markdown("üöß on development üöß")
